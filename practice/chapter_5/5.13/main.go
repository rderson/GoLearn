// Copyright Â© 2016 Alan A. A. Donovan & Brian W. Kernighan.
// License: https://creativecommons.org/licenses/by-nc-sa/4.0/

// See page 139.

// Findlinks3 crawls the web, starting with the URLs on the command line.
package main

import (
	"fmt"
	"io"
	"log"
	"net/http"
	"net/url"
	"os"
	"path/filepath"
	"strings"

	"practice/chapter_5/links"
)

// !+breadthFirst
// breadthFirst calls f for each item in the worklist.
// Any items returned by f are added to the worklist.
// f is called at most once for each item.
func breadthFirst(f func(item string) []string, worklist []string) {
	seen := make(map[string]bool)
	for len(worklist) > 0 {
		items := worklist
		worklist = nil
		for _, item := range items {
			if !seen[item] {
				seen[item] = true
				worklist = append(worklist, f(item)...)
			}
		}
	}
}

//!-breadthFirst

func savePage(urlStr string, content []byte) error {
	parsedURL, err := url.Parse(urlStr)
	if err != nil {
		return fmt.Errorf("error parsing url: %v", err)
	}

	filePath := filepath.Join(parsedURL.Host, parsedURL.Path)
	if parsedURL.Path == "" || strings.HasSuffix(parsedURL.Path, "/") {
		filePath = filepath.Join(filePath, "index.html")
	} else {
		if ext := filepath.Ext(filePath); ext == "" {
			filePath += ".html"
		}
	}

	if err := os.MkdirAll(filepath.Dir(filePath), 0755); err != nil {
		return fmt.Errorf("error creating directory: %v", err)
	}

	return os.WriteFile(filePath, content, 0644)
}

// !+crawl
func crawl(urlStr string, rootDomain string) []string {
	parsedURL, err := url.Parse(urlStr)
	if err != nil {
		log.Printf("error parsing url: %v\n", err)
		return nil
	}

	if parsedURL.Host != rootDomain {
		log.Printf("skipping URL %s from a different domain\n", urlStr)
		return nil
	}

	resp, err := http.Get(urlStr)
	if err != nil {
		log.Printf("error getting url: %v\n", err)
		return nil
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		log.Printf("getting %s: %s\n", urlStr, resp.Status)
		return nil
	}

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		log.Printf("error reading response body: %v\n", err)
		return nil
	}

	if err := savePage(urlStr, body); err != nil {
		log.Printf("error saving page %s: %v\n", urlStr, err)
	}

	list, err := links.Extract(urlStr)
	if err != nil {
		log.Printf("error extracting the links: %v\n", err)
		return nil
	}

	return list
}

//!-crawl

// !+main
func main() {
	if len(os.Args) < 2 {
		log.Fatal("usage: main.go <url>")
	}

	startURL := os.Args[1]
	if startURL == "https://golang.org" {
		startURL = "https://go.dev"
	}

	parsedURL, err := url.Parse(startURL)
	if err != nil {
		log.Fatalf("start url parsing error: %v\n", err)
	}

	rootDomain := parsedURL.Host

	breadthFirst(func(url string) []string {
		return crawl(url, rootDomain)
	}, os.Args[1:])
}

//!-main
